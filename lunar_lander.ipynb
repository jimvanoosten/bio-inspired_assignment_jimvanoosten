{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511f1834",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from time import time\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2284ac",
   "metadata": {},
   "source": [
    "# Create Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state = env.reset(seed=42)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c286a",
   "metadata": {},
   "source": [
    "# Random Action Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22509e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "actions = range(env.action_space.n)\n",
    "for i in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    while True:\n",
    "        action = np.random.choice(actions)\n",
    "        state, reward, done, info, a = env.step(action)\n",
    "        score += reward\n",
    "        if done:\n",
    "            if i % 20 == 0:\n",
    "                print('Episode {},  score: {}'.format(i, score))\n",
    "            break\n",
    "    \n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(max_episodes), scores)\n",
    "plt.title('Performance of Random Agent')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9d495",
   "metadata": {},
   "source": [
    "# Average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d03df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average score\n",
    "print('Average score of random agent over {} episodes: {:.2f}'.format(max_episodes, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c7d92",
   "metadata": {},
   "source": [
    "# Deep Q Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33baff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cuda if available else use cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b2969",
   "metadata": {},
   "source": [
    "# Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Build a fully connected neural network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state_size (int): State dimension\n",
    "        action_size (int): Action dimension\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0d7ba",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92914a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"\n",
    "        Replay memory allow agent to record experiences and learn from them\n",
    "        \n",
    "        Parametes\n",
    "        ---------\n",
    "        buffer_size (int): maximum size of internal memory\n",
    "        batch_size (int): sample size from experience\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience\"\"\"\n",
    "        experience = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "                \n",
    "    def sample(self):\n",
    "        \"\"\" \n",
    "        Sample randomly and return (state, action, reward, next_state, done) tuple as torch tensors \n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(device)        \n",
    "        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(device)        \n",
    "        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(device)  \n",
    "        # Convert done from boolean to int\n",
    "        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(device)        \n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d2fad",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfd76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5) # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 1e-3              # Soft update parameter for updating fixed q network\n",
    "LR = 1e-4               # Q Network learning rate\n",
    "UPDATE_EVERY = 4        # How often to update Q network\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        DQN Agent interacts with the environment, \n",
    "        stores the experience and learns from it\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state_size (int): Dimension of state\n",
    "        action_size (int): Dimension of action\n",
    "        seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        # Initialize Q and Fixed Q networks\n",
    "        self.q_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.fixed_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
    "        # Initiliase memory \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.timestep = 0\n",
    "        \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Agent's knowledge\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state (array_like): Current state of environment\n",
    "        action (int): Action taken in current state\n",
    "        reward (float): Reward received after taking action \n",
    "        next_state (array_like): Next state returned by the environment after taking action\n",
    "        done (bool): whether the episode ended after taking action\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestep += 1\n",
    "        if self.timestep % UPDATE_EVERY == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                sampled_experiences = self.memory.sample()\n",
    "                self.learn(sampled_experiences)\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        \"\"\"\n",
    "        Learn from experience by training the q_network \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        experiences (array_like): List of experiences sampled from agent's memory\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # Get the action with max Q value\n",
    "        action_values = self.fixed_network(next_states).detach()\n",
    "        # Notes\n",
    "        # tensor.max(1)[0] returns the values, tensor.max(1)[1] will return indices\n",
    "        # unsqueeze operation --> np.reshape\n",
    "        # Here, we make it from torch.Size([64]) -> torch.Size([64, 1])\n",
    "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # If done just use reward, else update Q_target with discounted action values\n",
    "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
    "        Q_expected = self.q_network(states).gather(1, actions)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(Q_expected, Q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update fixed weights\n",
    "        self.update_fixed_network(self.q_network, self.fixed_network)\n",
    "        \n",
    "    def update_fixed_network(self, q_network, fixed_network):\n",
    "        \"\"\"\n",
    "        Update fixed network by copying weights from Q network using TAU param\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        q_network (PyTorch model): Q network\n",
    "        fixed_network (PyTorch model): Fixed target network\n",
    "        \"\"\"\n",
    "        for source_parameters, target_parameters in zip(q_network.parameters(), fixed_network.parameters()):\n",
    "            target_parameters.data.copy_(TAU * source_parameters.data + (1.0 - TAU) * target_parameters.data)\n",
    "        \n",
    "        \n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"\n",
    "        Choose the action\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state (array_like): current state of environment\n",
    "        eps (float): epsilon for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        rnd = random.random()\n",
    "        if rnd < eps:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            # set the network into evaluation mode \n",
    "            self.q_network.eval()\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            # Back to training mode\n",
    "            self.q_network.train()\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            return action    \n",
    "        \n",
    "    def checkpoint(self, filename):\n",
    "        torch.save(self.q_network.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612429c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ff056",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5) # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 1e-3              # Soft update parameter for updating fixed q network\n",
    "LR = 1e-4               # Q Network learning rate\n",
    "UPDATE_EVERY = 4        # How often to update Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81673f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 2000  # Max number of episodes to play\n",
    "MAX_STEPS = 1000     # Max steps allowed in a single episode/play\n",
    "ENV_SOLVED = 200     # MAX score at which we consider environment to be solved\n",
    "PRINT_EVERY = 100    # How often to print the progress\n",
    "\n",
    "# Epsilon schedule\n",
    "\n",
    "EPS_START = 1.0      # Default/starting value of eps\n",
    "EPS_DECAY = 0.999    # Epsilon decay rate\n",
    "EPS_MIN = 0.01       # Minimum epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f375b0",
   "metadata": {},
   "source": [
    "# Visualise Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_DECAY_RATES = [0.9, 0.99, 0.999, 0.9999]\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for decay_rate in EPS_DECAY_RATES:\n",
    "    test_eps = EPS_START\n",
    "    eps_list = []\n",
    "    for _ in range(MAX_EPISODES):\n",
    "        test_eps = max(test_eps * decay_rate, EPS_MIN)\n",
    "        eps_list.append(test_eps)          \n",
    "    \n",
    "    plt.plot(eps_list, label='decay rate: {}'.format(decay_rate))\n",
    "\n",
    "plt.title('Effect of various decay rates')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('# of episodes')\n",
    "plt.ylabel('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('State size: {}, action size: {}'.format(state_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450123aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(state_size, action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ffeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "scores = []\n",
    "# Maintain a list of last 100 scores\n",
    "scores_window = deque(maxlen=100)\n",
    "eps = EPS_START\n",
    "for episode in range(1, MAX_EPISODES + 1):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for t in range(MAX_STEPS):\n",
    "        action = dqn_agent.act(state, eps)\n",
    "        next_state, reward, done, info, a = env.step(action)\n",
    "        dqn_agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        eps = max(eps * EPS_DECAY, EPS_MIN)\n",
    "        if episode % PRINT_EVERY == 0:\n",
    "            mean_score = np.mean(scores_window)\n",
    "            print('\\r Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score), end=\"\")\n",
    "        if score >= ENV_SOLVED:\n",
    "            mean_score = np.mean(scores_window)\n",
    "            print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            dqn_agent.checkpoint('solved_200.pth')\n",
    "            break\n",
    "            \n",
    "    scores_window.append(score)\n",
    "    scores.append(score)\n",
    "    \n",
    "end = time()    \n",
    "print('Took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd546768",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.checkpoint('solved_200.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376625d6",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27120fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(scores)\n",
    "# A bit hard to see the above plot, so lets smooth it (red)\n",
    "plt.plot(pd.Series(scores).rolling(100).mean())\n",
    "plt.title('DQN Training')\n",
    "plt.xlabel('# of episodes')\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126555c7",
   "metadata": {},
   "source": [
    "# Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.q_network.load_state_dict(torch.load('solved_200.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a84129",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = dqn_agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "    print('episode: {} scored {}'.format(i, score))\n",
    "    \n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f5c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
